!pip install langchain==0.3.1 langchain-community==0.3.1 langchain-text-splitters langchain-huggingface faiss-cpu PyPDF2 python-docx sentence-transformers
import os
import faiss
import numpy as np
from io import BytesIO
from PyPDF2 import PdfReader
from docx import Document

from langchain_text_splitters import CharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_community.document_loaders import WebBaseLoader

# Conversational chain (IMPORTANT)
from langchain.chains import ConversationalRetrievalChain

# ChatModel (THIS IS THE FIX)
from langchain_huggingface import ChatHuggingFace
from getpass import getpass

huggingface_api_key = getpass("Enter your HuggingFace API key: ")
os.environ['HUGGINGFACEHUB_API_TOKEN'] = huggingface_api_key
def process_input(input_type, input_data):
    text = ""

    if input_type == "PDF":
        for f in input_data:
            pdf = PdfReader(f)
            for page in pdf.pages:
                t = page.extract_text()
                if t:
                    text += t + "\n"

    elif input_type == "Text":
        text = input_data

    splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    chunks = splitter.split_text(text)

    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-mpnet-base-v2",
        model_kwargs={"device": "cpu"}
    )

    dim = len(embeddings.embed_query("hello"))
    index = faiss.IndexFlatL2(dim)

    vectorstore = FAISS(
        embedding_function=embeddings.embed_query,
        index=index,
        docstore=InMemoryDocstore(),
        index_to_docstore_id={}
    )

    vectorstore.add_texts(chunks)
    return vectorstore
from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint
from langchain.chains import ConversationalRetrievalChain

def build_llama_conversational_chain(vectorstore):

    endpoint = HuggingFaceEndpoint(
        repo_id="meta-llama/Meta-Llama-3-8B-Instruct",
        task="conversational",
        temperature=0.5,
        max_new_tokens=512,
        token=huggingface_api_key
    )

    llm = ChatHuggingFace(
        llm=endpoint   # REQUIRED
    )

    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectorstore.as_retriever(),
        return_source_documents=False
    )
    return chain
def chat_with_pdf(chain):
    chat_history = []

    while True:
        q = input("Ask: ")
        if q.lower() == "exit":
            break

        result = chain({"question": q, "chat_history": chat_history})
        ans = result["answer"]

        print("\nAI:", ans)
        chat_history.append((q, ans))
from google.colab import files

uploaded = files.upload()
pdf_files = [BytesIO(data) for data in uploaded.values()]

vs = process_input("PDF", pdf_files)
chain = build_llama_conversational_chain(vs)
chat_with_pdf(chain)
